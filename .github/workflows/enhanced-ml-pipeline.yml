name: Enhanced ML Production Pipeline

on:
  pull_request:
    branches: [ main ]
  schedule:
    # Run regression tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
        - full
        - performance-only

env:
  AWS_REGION: eu-west-2
  ECR_GENERATE_REPO: transformer-model-generate-text
  ECR_VISUALIZE_REPO: transformer-model-visualize-attention
  PROD_API_BASE: https://0fc0dgwg69.execute-api.eu-west-2.amazonaws.com
  MODEL_BUCKET_PREFIX: transformer-model-artifacts
  TERRAFORM_STATE_BUCKET: dom-terraform-state-bucket

jobs:
  # =============================================================================
  # CODE QUALITY & SECURITY SCANNING
  # =============================================================================
  
  code-quality-security:
    name: Code Quality & Security Analysis
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install analysis tools
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort bandit safety mypy pytest

      - name: Code formatting check (Black)
        run: |
          black --check --diff src/
          echo "‚úÖ Code formatting check passed"

      - name: Import sorting check (isort)
        run: |
          isort --check-only --diff src/
          echo "‚úÖ Import sorting check passed"

      - name: Linting (flake8) - Lambda functions
        run: |
          # Check for syntax errors and undefined names
          flake8 src/lambda_functions/ --count --select=E9,F63,F7,F82 --show-source --statistics
          # Check complexity and style (warnings only)
          flake8 src/lambda_functions/ --count --exit-zero --max-complexity=10 --max-line-length=100 --statistics
          echo "‚úÖ Lambda functions linting passed"

      - name: Linting (flake8) - Streamlit app
        run: |
          flake8 src/streamlit/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src/streamlit/ --count --exit-zero --max-complexity=12 --max-line-length=100 --statistics
          echo "‚úÖ Streamlit app linting passed"

      - name: Security scan (Bandit) - Lambda functions
        run: |
          bandit -r src/lambda_functions/ -f json -o bandit-lambda-report.json || true
          echo "Bandit report contents:"
          cat bandit-lambda-report.json
          bandit -r src/lambda_functions/ --severity-level medium || true
          echo "‚úÖ Lambda security scan passed"

      - name: Security scan (Bandit) - Streamlit app
        run: |
          bandit -r src/streamlit/ -f json -o bandit-streamlit-report.json || true
          echo "Bandit Streamlit report contents:"
          cat bandit-streamlit-report.json
          bandit -r src/streamlit/ --severity-level medium || true
          echo "‚úÖ Streamlit security scan passed"

      - name: Dependency vulnerability scan - Generate text
        run: |
          pip install -r src/lambda_functions/generate_text/requirements.txt
          safety scan || echo "‚ö†Ô∏è WARNING: Security vulnerabilities found in dependencies"
          echo "‚úÖ Generate text dependencies scan completed (with warnings)"

      - name: Dependency vulnerability scan - Visualize attention  
        run: |
          pip install -r src/lambda_functions/visualize_attention/requirements.txt
          safety scan || echo "‚ö†Ô∏è WARNING: Security vulnerabilities found in dependencies"
          echo "‚úÖ Visualize attention dependencies scan completed (with warnings)"

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-*-report.json
            safety-*-report.json

  # =============================================================================
  # LAMBDA FUNCTION UNIT TESTS
  # =============================================================================
  
  lambda-unit-tests:
    name: Lambda Function Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality-security
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install test dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-mock moto[s3,lambda] boto3
          # Install Lambda dependencies
          pip install -r src/lambda_functions/generate_text/requirements.txt
          pip install -r src/lambda_functions/visualize_attention/requirements.txt

      - name: Test tokenizer functionality
        run: |
          pytest tests/unit/test_tokenizer.py -v --cov=src/lambda_functions --cov-report=xml
          echo "‚úÖ Tokenizer tests passed"

      - name: Test transformer model components
        run: |
          pytest tests/unit/test_transformer.py -v --cov-append --cov=src/lambda_functions --cov-report=xml
          echo "‚úÖ Transformer model tests passed"

      - name: Test Lambda handlers
        run: |
          pytest tests/unit/test_lambda_handlers.py -v --cov-append --cov=src/lambda_functions --cov-report=xml
          echo "‚úÖ Lambda handler tests passed"

      - name: Test S3 model loading
        run: |
          pytest tests/unit/test_s3_integration.py -v --cov-append --cov=src/lambda_functions --cov-report=xml
          echo "‚úÖ S3 integration tests passed"

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: lambda-unittests
          name: codecov-lambda

  # =============================================================================
  # INFRASTRUCTURE VALIDATION
  # =============================================================================
  
  infrastructure-validation:
    name: Terraform Infrastructure Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Terraform Format Check
        run: |
          terraform fmt -check -recursive terraform/
          echo "‚úÖ Terraform formatting check passed"

      - name: Terraform Validation - Main
        run: |
          cd terraform/
          terraform init -backend=false
          terraform validate
          echo "‚úÖ Main Terraform validation passed"

      - name: Terraform Validation - Modules
        run: |
          for module in terraform/*/; do
            if [ -f "$module/main.tf" ]; then
              echo "Validating module: $module"
              cd "$module"
              terraform init -backend=false
              terraform validate
              cd - > /dev/null
            fi
          done
          echo "‚úÖ All Terraform modules validated"

      # TODO: Re-enable after fixing GitHub Issues #1 and #2
      # - name: TFSec Security Scan
      #   uses: aquasecurity/tfsec-action@v1.0.3
      #   with:
      #     working_directory: terraform/
      #     format: sarif

      - name: Checkov Infrastructure Security Scan
        uses: bridgecrewio/checkov-action@master
        with:
          directory: terraform/
          framework: terraform
          output_format: sarif
          output_file_path: checkov-results.sarif

      - name: Upload infrastructure security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: infrastructure-security-reports
          path: |
            tfsec-results.sarif
            checkov-results.sarif

  # =============================================================================
  # CONTAINER SECURITY SCANNING
  # =============================================================================
  
  container-security:
    name: Container Security & Vulnerability Scanning
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build Generate Text Lambda container
        run: |
          docker build -t generate-text-test:latest \
            -f src/lambda_functions/generate_text/Dockerfile \
            src/lambda_functions/generate_text/

      - name: Build Visualize Attention Lambda container
        run: |
          docker build -t visualize-attention-test:latest \
            -f src/lambda_functions/visualize_attention/Dockerfile \
            src/lambda_functions/visualize_attention/

      - name: Run Trivy vulnerability scanner - Generate Text
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: generate-text-test:latest
          format: 'sarif'
          output: 'generate-text-trivy.sarif'

      - name: Run Trivy vulnerability scanner - Visualize Attention
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: visualize-attention-test:latest
          format: 'sarif' 
          output: 'visualize-attention-trivy.sarif'

      - name: Test container functionality
        run: |
          # Test basic container startup
          timeout 30s docker run --rm --entrypoint python generate-text-test:latest -c "import torch; import boto3; print('Generate text container OK')"
          timeout 30s docker run --rm --entrypoint python visualize-attention-test:latest -c "import torch; import matplotlib; print('Visualize attention container OK')"
          echo "‚úÖ Container functionality tests passed"

      - name: Upload container security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: container-security-reports
          path: |
            *-trivy.sarif

  # =============================================================================
  # BUILD & DEPLOY (MAIN BRANCH ONLY)
  # =============================================================================
  
  build-and-deploy:
    name: Build and Deploy to Production
    runs-on: ubuntu-latest
    needs: [lambda-unit-tests, infrastructure-validation, container-security]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and push Generate Text image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build and tag image
          docker build -t $ECR_REGISTRY/$ECR_GENERATE_REPO:$IMAGE_TAG \
            -f src/lambda_functions/generate_text/Dockerfile .
          docker build -t $ECR_REGISTRY/$ECR_GENERATE_REPO:latest \
            -f src/lambda_functions/generate_text/Dockerfile .
          
          # Push both tags
          docker push $ECR_REGISTRY/$ECR_GENERATE_REPO:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_GENERATE_REPO:latest
          
          echo "generate_image_uri=$ECR_REGISTRY/$ECR_GENERATE_REPO:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Build and push Visualize Attention image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build and tag image
          docker build -t $ECR_REGISTRY/$ECR_VISUALIZE_REPO:$IMAGE_TAG \
            -f src/lambda_functions/visualize_attention/Dockerfile .
          docker build -t $ECR_REGISTRY/$ECR_VISUALIZE_REPO:latest \
            -f src/lambda_functions/visualize_attention/Dockerfile .
          
          # Push both tags
          docker push $ECR_REGISTRY/$ECR_VISUALIZE_REPO:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_VISUALIZE_REPO:latest
          
          echo "visualize_image_uri=$ECR_REGISTRY/$ECR_VISUALIZE_REPO:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

  # =============================================================================
  # PRODUCTION INTEGRATION TESTS
  # =============================================================================
  
  production-integration-tests:
    name: Production Integration & Performance Tests
    runs-on: ubuntu-latest
    needs: build-and-deploy
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install test dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest requests boto3 numpy

      - name: Wait for deployment stabilization
        run: |
          echo "‚è≥ Waiting 120 seconds for Lambda functions to stabilize..."
          sleep 120

      - name: Test Generate Text API endpoint
        run: |
          pytest tests/integration/test_generate_text_api.py -v \
            --api-base=${{ env.PROD_API_BASE }}
          echo "‚úÖ Generate text API tests passed"

      - name: Test Visualize Attention API endpoint
        run: |
          pytest tests/integration/test_visualize_attention_api.py -v \
            --api-base=${{ env.PROD_API_BASE }}
          echo "‚úÖ Visualize attention API tests passed"

      - name: End-to-end workflow test
        run: |
          pytest tests/integration/test_e2e_workflow.py -v \
            --api-base=${{ env.PROD_API_BASE }}
          echo "‚úÖ End-to-end workflow tests passed"

      - name: Performance benchmark tests
        run: |
          python tests/performance/test_lambda_performance.py \
            --api-base=${{ env.PROD_API_BASE }}
          echo "‚úÖ Performance benchmark tests completed"

      - name: Cold start performance analysis
        run: |
          python tests/performance/test_cold_start_analysis.py \
            --api-base=${{ env.PROD_API_BASE }}
          echo "‚úÖ Cold start analysis completed"

  # =============================================================================
  # COST & MONITORING SETUP
  # =============================================================================
  
  setup-monitoring-alerts:
    name: Configure Production Monitoring & Cost Alerts
    runs-on: ubuntu-latest
    needs: production-integration-tests
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install AWS SDK
        run: |
          pip install boto3

      - name: Setup CloudWatch custom metrics
        run: |
          python scripts/setup_custom_metrics.py
          echo "‚úÖ Custom metrics configured"

      # - name: Configure cost monitoring alerts
      #   run: |
      #     python scripts/setup_cost_alerts.py \
      #       --threshold=10 \
      #       --email=${{ secrets.ALERT_EMAIL }}
      #     echo "‚úÖ Cost alerts configured"

      - name: Setup performance dashboards
        run: |
          python scripts/setup_cloudwatch_dashboards.py \
            --project-name=transformer-model
          echo "‚úÖ CloudWatch dashboards configured"

      - name: Generate deployment report
        run: |
          python scripts/generate_deployment_report.py \
            --commit-sha=${{ github.sha }} \
            --deployment-time="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "‚úÖ Deployment report generated"

  # =============================================================================
  # DEPLOYMENT NOTIFICATION & REPORTING
  # =============================================================================
  
  deployment-notification:
    name: Deployment Notification & Reporting
    runs-on: ubuntu-latest
    needs: [production-integration-tests, setup-monitoring-alerts]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Determine deployment status
        id: deployment-status
        run: |
          if [[ "${{ needs.production-integration-tests.result }}" == "success" && "${{ needs.setup-monitoring-alerts.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=üöÄ Production deployment successful! All systems operational." >> $GITHUB_OUTPUT
            echo "color=good" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=‚ùå Production deployment failed. Manual intervention required." >> $GITHUB_OUTPUT
            echo "color=danger" >> $GITHUB_OUTPUT
          fi

      - name: Get deployment metrics
        id: metrics
        run: |
          echo "commit_author=${{ github.actor }}" >> $GITHUB_OUTPUT
          echo "commit_message=$(git log --format=%B -n 1 ${{ github.sha }} | head -1)" >> $GITHUB_OUTPUT
          echo "deployment_time=$(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)" >> $GITHUB_OUTPUT

      # - name: Send Slack deployment notification
      #   uses: 8398a7/action-slack@v3
      #   if: always()
      #   with:
      #     status: ${{ steps.deployment-status.outputs.status }}
      #     custom_payload: |
      #       {
      #         "text": "${{ steps.deployment-status.outputs.message }}",
      #         "attachments": [
      #           {
      #             "color": "${{ steps.deployment-status.outputs.color }}",
      #             "fields": [
      #               {
      #                 "title": "üèóÔ∏è Project",
      #                 "value": "Custom Transformer ML Pipeline",
      #                 "short": true
      #               },
      #               {
      #                 "title": "üåç Environment", 
      #                 "value": "Production (AWS eu-west-2)",
      #                 "short": true
      #               },
      #               {
      #                 "title": "üë§ Author",
      #                 "value": "${{ steps.metrics.outputs.commit_author }}",
      #                 "short": true
      #               },
      #               {
      #                 "title": "‚è∞ Deployed",
      #                 "value": "${{ steps.metrics.outputs.deployment_time }}",
      #                 "short": true
      #               },
      #               {
      #                 "title": "üîó Commit",
      #                 "value": "<https://github.com/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>",
      #                 "short": true
      #               },
      #               {
      #                 "title": "üåê Live URL",
      #                 "value": "<${{ env.PROD_API_BASE }}|Production API>",
      #                 "short": true
      #               }
      #             ]
      #           }
      #         ]
      #       }
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  # =============================================================================
  # SCHEDULED REGRESSION & MONITORING TESTS
  # =============================================================================
  
  scheduled-regression-tests:
    name: Scheduled Production Health & Regression Tests
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 2 * * *'  # Daily at 2 AM UTC
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest requests boto3 numpy pandas

      - name: Run daily health checks
        run: |
          pytest tests/regression/test_daily_health_checks.py -v \
            --api-base=${{ env.PROD_API_BASE }}
          echo "‚úÖ Daily health checks completed"

      - name: Model performance regression test
        run: |
          python tests/regression/test_model_performance_regression.py \
            --api-base=${{ env.PROD_API_BASE }} \
            --baseline-file=tests/data/performance_baseline.json
          echo "‚úÖ Model performance regression test completed"

      - name: API response time monitoring
        run: |
          python tests/regression/test_api_response_times.py \
            --api-base=${{ env.PROD_API_BASE }} \
            --max-cold-start=30 \
            --max-warm-response=5
          echo "‚úÖ API response time monitoring completed"

      - name: Cost drift analysis
        run: |
          python tests/regression/test_cost_drift_analysis.py \
            --region=${{ env.AWS_REGION }} \
            --project-prefix=transformer-model
          echo "‚úÖ Cost drift analysis completed"

      - name: Generate regression report
        run: |
          python scripts/generate_regression_report.py \
            --output-dir=reports/regression \
            --date=$(date +%Y-%m-%d)
          echo "‚úÖ Regression report generated"

      - name: Upload regression artifacts
        uses: actions/upload-artifact@v4
        with:
          name: regression-results-${{ github.run_id }}
          path: |
            reports/regression/
            tests/logs/